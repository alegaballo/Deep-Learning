{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1 style=\"text-align:center\">Deep Learning  Lab Session </h1>\n",
    "<h1 style=\"text-align:center\">First Lab Session - 3 Hours </h1>\n",
    "<h1 style=\"text-align:center\">Artificial Neural Networks for Handwritten Digits Recognition</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b> Student 1:</b> Alessandro Gaballo \n",
    "<b> Student 2:</b> Jonas Wacker\n",
    " \n",
    " \n",
    "The aim of this session is to practice with Artificial Neural Networks. Answers and experiments should be made by groups of one or two students. Each group should fill and run appropriate notebook cells. \n",
    "\n",
    "To generate your final report, use print as PDF (Ctrl+P). Do not forget to run all your cells before generating your final report and do not forget to include the names of all participants in the group. The lab session should be completed by April 7th 2017. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "In this session, your will implement, train and test a Neural Network\n",
    "for the Handwritten Digits Recognition problem <a href=\"http://yann.lecun.com/exdb/mnist/\"> [1] </a> with  different settings of hyper parameters. You will use the MNIST dataset which was constructed from a number of scanned document dataset available from the National Institute of Standards and Technology (NIST). Images of digits were taken from a variety of scanned documents, normalized in size and centered. \n",
    "\n",
    "\n",
    "<img src=\"Nimages/mnist.png\",width=\"350\" height=\"500\" align=\"center\">\n",
    "<center><span>Figure 1: MNIST digits examples</span></center>\n",
    "\n",
    "\n",
    "This assignment includes a written part of programms to help you understand how to build and train\n",
    "your neural net and then to test your code and get restults. \n",
    "\n",
    "1. <a href=\"NeuralNetwork.py\"> NeuralNetwork.py </a> \n",
    "2. <a href=\"transfer_functions.py\"> transfer_functions.py </a> \n",
    "3.  <a href=\"utils.py \"> utils.py </a> \n",
    "\n",
    "\n",
    "Functions defined inside the python files mentionned above can be imported  using the python command : \n",
    "from filename import *\n",
    "\n",
    "You will use the following libraries:\n",
    "\n",
    "1. <a href=\"http://cs231n.github.io/python-numpy-tutorial/\"> numpy </a>: for creating arrays and using methods to manipulate arrays.\n",
    "\n",
    "2. <a href=\"http://matplotlib.org/\"> matplotlib  </a>: for making plots\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Section 1 :  My First Neural Network\n",
    "## DO NOT RUN AGAIN, THE OUTPUT IS THERE AND IT'S FINE\n",
    "\n",
    "<b>Part 1</b>: Before designing and writing your code, you will first work on a neural network by hand. \n",
    "Consider the above Neural network with two inputs $X=(x1,x2)$, one hidden layers and a single output unit $(y)$.\n",
    "The initial weights are set to random values. Neurons 6 and 7 represent the bias. Bias values are equal to 1.  \n",
    "Training sample, X = (0.8, 0.2), whose class label is Y=0.4.\n",
    "\n",
    "Assume that the neurons have a Sigmoid activation function  $f(x)=\\frac{1}{(1+e^{-x})}$ and the learning rate $\\mu$=1\n",
    "\n",
    "\n",
    "<img src=\"Nimages/NN.png\", width=\"700\" height=\"900\"> \n",
    "<center><span>Figure 2: Neural network </span></center>\n",
    "\n",
    "\n",
    "<b>Question 1.1.1</b>: Compute the new values of weights $w_{i,j}$ after a forward pass and a backward pass.\n",
    "$w_{i,j}$ is the weight of the connexion between neuron $i$ and neuron $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#Your answer goes here :\n",
    "\n",
    "$w_{1,3}=  0.3043 $ \n",
    "\n",
    "$w_{1,4}=  -0.5027 $\n",
    "\n",
    "$w_{2,3}= 0.8011 $\n",
    "\n",
    "$w_{2,4}= 0.1993 $\n",
    "\n",
    "$w_{6,3}= 0.2054 $\n",
    "\n",
    "$w_{6,4}= -0.4034 $\n",
    "\n",
    "$w_{3,5}= 0.6254 $\n",
    "\n",
    "$w_{4,5}= 0.3875 $\n",
    "\n",
    "$w_{7,5}= 0.4606 $\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>Part 2</b>: Neural Network Implementation\n",
    "\n",
    "Please read all source files carefully and understand the data structures and all functions.\n",
    "You are to complete the missing code. \n",
    "First you should define the neural network (using the NeuralNetwork class, see in the <a href=\"NeuralNetwork.py\"> NeuralNetwork.py </a> file) and reinitialise weights. \n",
    "Then you will to complete the Feed Forward and the Back-propagation functions. \n",
    "\n",
    "<b>Question 1.2.1</b>: Define the neural network corresponding to the one in part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from NeuralNetwork import *\n",
    "import numpy as np\n",
    "#create the network\n",
    "my_first_net = NeuralNetwork(2, 2, 1, learning_rate=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.3 -0.5]\n",
      " [ 0.8  0.2]\n",
      " [ 0.2 -0.4]]\n",
      "[[-0.6]\n",
      " [ 0.4]\n",
      " [ 0.5]]\n"
     ]
    }
   ],
   "source": [
    "#Data preparation \n",
    "X=[0.8,0.2]\n",
    "Y=[0.4]\n",
    "data=[]\n",
    "data.append(X)\n",
    "data.append(Y)\n",
    "\n",
    "#initialize weights\n",
    "wi=np.array([[0.3,-0.5],[0.8,0.2],[0.2,-0.4]])\n",
    "wo=np.array([[-0.6],[0.4],[0.5]])\n",
    "my_first_net.weights_initialisation(wi,wo)\n",
    "print(my_first_net.W_input_to_hidden)\n",
    "print(my_first_net.W_hidden_to_output)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>Question 1.2.2</b>: Implement the Feed Forward function (feedForward(X) in the NeuralNetwork.py file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Implement it in the NeuralNetwork.py file and when finalised copy and paste your FeedForward function here\n",
    "def transfer_function(self, x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "    \n",
    "def feedForward(self, inputs):\n",
    "    self.a_input = np.append(inputs, 1)\n",
    "    a_hidden_without_bias = np.dot(self.a_input, self.W_input_to_hidden)\n",
    "    self.a_hidden = np.append(a_hidden_without_bias, 0)\n",
    "    self.o_hidden = self.transfer_function(self.a_hidden)\n",
    "    self.o_hidden[-1] = 1\n",
    "    self.a_output = np.dot(self.o_hidden, self.W_hidden_to_output)\n",
    "    self.o_output = self.transfer_function(self.a_output)\n",
    "    return self.o_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Check your network outputs the expected value (the one you computed in question 1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output activation =0.560\n"
     ]
    }
   ],
   "source": [
    "#test my  Feed Forward function \n",
    "Output_activation=my_first_net.feedForward(X)\n",
    "print(\"output activation =%.3f\" %(Output_activation))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>Question 1.2.3</b>: Implement the Back-propagation Algorithm (backPropagate(Y) in the NeuralNetwork.py file)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Implement it in the NeuralNetwork.py file and when finalised copy and paste your BackPropagate function here\n",
    "def backPropagate(self, targets):\n",
    "        \n",
    "    # calculate error terms for output\n",
    "    self.errors = self.o_output - targets\n",
    "    delta_e_u_output = self.errors * self.o_output * (1 - self.o_output)\n",
    "    delta_e_u_horizontal = np.matrix(delta_e_u_output)\n",
    "    o_hidden_vertical = np.matrix(self.o_hidden).T\n",
    "        \n",
    "    delta_e_w_output = np.dot(o_hidden_vertical, delta_e_u_horizontal)\n",
    "\n",
    "    # calculate error terms for hidden\n",
    "    delta_e_u_hidden = np.dot(self.W_hidden_to_output, delta_e_u_output) * self.o_hidden * (1 - self.o_hidden)\n",
    "    delta_e_u_horizontal = np.matrix(delta_e_u_hidden)\n",
    "    o_input_vertical = np.matrix(self.a_input).T\n",
    "    delta_e_w_hidden = np.dot(o_input_vertical, delta_e_u_horizontal)\n",
    "    # delete last column\n",
    "    # delta_e_w_hidden = delta_e_w_hidden[:,0:delta_e_w_hidden.shape[1]-1]\n",
    "    delta_e_w_hidden = np.delete(delta_e_w_hidden, -1, 1)\n",
    "    # update output weights\n",
    "    self.W_hidden_to_output -= self.learning_rate * delta_e_w_output\n",
    "    # update input weights\n",
    "    self.W_input_to_hidden -= self.learning_rate * delta_e_w_hidden\n",
    "        \n",
    "    return np.square(self.errors).sum()/2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Check the gradient values and weight updates are correct (similar to the ones you computed in question 1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New input weights\n",
      " [[ 0.30432265 -0.50273473]\n",
      " [ 0.80108066  0.19931632]\n",
      " [ 0.20540332 -0.40341841]]\n",
      "New output weights\n",
      " [[-0.62541468]\n",
      " [ 0.38745727]\n",
      " [ 0.46063746]]\n"
     ]
    }
   ],
   "source": [
    "#test my  Back-propagation function\n",
    "my_first_net.backPropagate(Y)\n",
    "#Print weights after backpropagation\n",
    "print('New input weights\\n', my_first_net.W_input_to_hidden)\n",
    "print('New output weights\\n', my_first_net.W_hidden_to_output)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Your Feed Forward and Back-Propagation implementations are working, Great!! Let's tackle a real world problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Section 2 : The MNIST Challenge! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>Data Preparation</b>\n",
    "\n",
    "The MNIST dataset consists of handwritten digit images it contains 60,000 examples for the training set and 10,000 examples for testing. In this Lab Session, the official training set of 60,000 is divided into an actual training set of 50,000 examples, 10,000 validation examples and 10,000 examples for test. All digit images have been size-normalized and centered in a fixed size image of 28 x 28 pixels. The images are stored in byte form you will use the NumPy python library to read the data files into NumPy arrays that we will use to train the ANN.\n",
    "\n",
    "The MNIST dataset is available in the Data folder.\n",
    "To get the training, testing and validation data, run the the load_data() function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST data .....\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "training_data, validation_data, test_data=load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>MNIST Dataset Digits Visualisation</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAADwCAYAAABVGPDuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBkpERKKkBkpERKKkBkpERKL0/516iF4ejbUOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe11e301a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ROW = 2\n",
    "COLUMN = 4\n",
    "for i in range(ROW * COLUMN):\n",
    "    # train[i][0] is i-th image data with size 28x28\n",
    "    image = training_data[i][0].reshape(28, 28)\n",
    "    plt.subplot(ROW, COLUMN, i+1)          \n",
    "    plt.imshow(image, cmap='gray')  # cmap='gray' is for black and white picture.\n",
    "plt.axis('off')  # do not show axis value\n",
    "plt.tight_layout()   # automatic padding between subplots\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>Part 1</b>: Creating the Neural Networks\n",
    "\n",
    "The input layer of the neural network contains neurons encoding the values of the input pixels. The training data for the network will consist of many 28 by 28 pixel images of scanned handwritten digits, and so the input layer contains 784=28Ã—28 neurons. The second layer of the network is a hidden layer, we set the neuron number in the hidden layer to 30. The output layer contains 10 neurons. \n",
    "\n",
    "<b>Question 2.1.1</b>: Create the network described above using the NeuralNetwork class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#create the network\n",
    "from NeuralNetwork import NeuralNetwork \n",
    "\n",
    "input_nodes = 784\n",
    "hidden_nodes = 30\n",
    "output_nodes = 10\n",
    "\n",
    "my_mnist_net = NeuralNetwork(input_nodes, hidden_nodes, output_nodes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>Question 2.1.2</b>: Add the information about the performance of the neural network on the test set at each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_accuracy=my_mnist_net.predict(test_data) / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_Accuracy [epoch = 0]  10.09\n"
     ]
    }
   ],
   "source": [
    "print('Test_Accuracy [epoch = 0]  %-2.2f' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>Question 2.1.3</b>: Train the Neural Network and comment your findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  1/50[==============] -Error: 0.1123316059  -Training_Accuracy:  92.67  -time: 42.63 \n",
      "Validation accuracy 92.7\n",
      "Iteration:  2/50[==============] -Error: 0.0560563344  -Training_Accuracy:  94.32  -time: 131.09 \n",
      "Validation accuracy 93.65\n",
      "Iteration:  3/50[==============] -Error: 0.0468937200  -Training_Accuracy:  95.34  -time: 217.78 \n",
      "Validation accuracy 94.5\n",
      "Iteration:  4/50[==============] -Error: 0.0417135517  -Training_Accuracy:  95.72  -time: 302.73 \n",
      "Validation accuracy 94.96\n",
      "Iteration:  5/50[==============] -Error: 0.0384278051  -Training_Accuracy:  96.22  -time: 409.63 \n",
      "Validation accuracy 95.17\n",
      "Iteration:  6/50[==============] -Error: 0.0357778918  -Training_Accuracy:  96.41  -time: 514.31 \n",
      "Validation accuracy 95.21\n",
      "Iteration:  7/50[==============] -Error: 0.0340840941  -Training_Accuracy:  96.35  -time: 568.56 \n",
      "Validation accuracy 95.03\n",
      "Iteration:  8/50[==============] -Error: 0.0322334574  -Training_Accuracy:  96.77  -time: 646.06 \n",
      "Validation accuracy 95.57\n",
      "Iteration:  9/50[==============] -Error: 0.0311007917  -Training_Accuracy:  96.83  -time: 709.60 \n",
      "Validation accuracy 95.64\n",
      "Iteration: 10/50[==============] -Error: 0.0299295064  -Training_Accuracy:  97.07  -time: 764.39 \n",
      "Validation accuracy 95.72\n",
      "Iteration: 11/50[==============] -Error: 0.0287945642  -Training_Accuracy:  97.10  -time: 869.94 \n",
      "Validation accuracy 95.69\n",
      "Iteration: 12/50[==============] -Error: 0.0279465594  -Training_Accuracy:  97.15  -time: 985.28 \n",
      "Validation accuracy 95.69\n",
      "Iteration: 13/50[==============] -Error: 0.0270195310  -Training_Accuracy:  97.17  -time: 1081.39 \n",
      "Validation accuracy 95.66\n",
      "Iteration: 14/50[==============] -Error: 0.0263753463  -Training_Accuracy:  97.33  -time: 1147.75 \n",
      "Validation accuracy 95.89999999999999\n",
      "Iteration: 15/50[==============] -Error: 0.0255188896  -Training_Accuracy:  97.50  -time: 1256.36 \n",
      "Validation accuracy 96.09\n",
      "Iteration: 16/50[==============] -Error: 0.0248074819  -Training_Accuracy:  97.48  -time: 1381.43 \n",
      "Validation accuracy 95.88\n",
      "Iteration: 17/50[==============] -Error: 0.0242756412  -Training_Accuracy:  97.55  -time: 1517.51 \n",
      "Validation accuracy 95.78\n",
      "Iteration: 18/50[==============] -Error: 0.0237267106  -Training_Accuracy:  97.65  -time: 1656.90 \n",
      "Validation accuracy 95.91\n",
      "Iteration: 19/50[==============] -Error: 0.0232591624  -Training_Accuracy:  97.67  -time: 1806.32 \n",
      "Validation accuracy 95.99\n",
      "Iteration: 20/50[==============] -Error: 0.0227244074  -Training_Accuracy:  97.77  -time: 1907.30 \n",
      "Validation accuracy 95.98\n",
      "Iteration: 21/50[==============] -Error: 0.0223126635  -Training_Accuracy:  97.80  -time: 1999.10 \n",
      "Validation accuracy 96.02000000000001\n",
      "Iteration: 22/50[==============] -Error: 0.0214598691  -Training_Accuracy:  97.81  -time: 2148.18 \n",
      "Validation accuracy 95.93\n",
      "Iteration: 23/50[==============] -Error: 0.0213472628  -Training_Accuracy:  97.94  -time: 2237.59 \n",
      "Validation accuracy 96.00999999999999\n",
      "Iteration: 24/50[==============] -Error: 0.0208509002  -Training_Accuracy:  97.88  -time: 2336.72 \n",
      "Validation accuracy 96.00999999999999\n",
      "Iteration: 25/50[==============] -Error: 0.0204703033  -Training_Accuracy:  98.01  -time: 2470.56 \n",
      "Validation accuracy 96.00999999999999\n",
      "Iteration: 26/50[==============] -Error: 0.0201351799  -Training_Accuracy:  98.07  -time: 2584.35 \n",
      "Validation accuracy 95.94\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'argmax'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-64aa82f82f25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#train your network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmy_mnist_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#save your model in Models/ using a distinguishing name for your model (architecture, learning rate, etc...)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/Documenti/Eurecom/Spring/deepLearning/labs/Deep-Learning/neural_networks/NeuralNetwork.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data, validation_data)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0merror\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackPropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0mTraining_accuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m             \u001b[0mVal_accuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/Documenti/Eurecom/Spring/deepLearning/labs/Deep-Learning/neural_networks/NeuralNetwork.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, test_data)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtestcase\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtestcase\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m             \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedForward\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtestcase\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     \"\"\"\n\u001b[0;32m--> 963\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# An AttributeError occurs if the object does not have\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_accuracy = []\n",
    "#train your network \n",
    "val_accuracies = my_mnist_net.train(training_data,validation_data)\n",
    "#save your model in Models/ using a distinguishing name for your model (architecture, learning rate, etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "my_mnist_net.save('NN_MNIST_default_params')\n",
    "test_accuracy.append(my_mnist_net.predict(test_data)/100)\n",
    "print('Test_Accuracy  %-2.2f' % test_accuracy[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#Note your observations here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>Question 2.1.4</b>: Guess digit, Implement and test a python function that predict the class of a digit (the folder images_test contains some examples of images of digits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Your implementation goes here\n",
    "\n",
    "#DON'T KNOW HOW TO READ THE IMG\n",
    "import os\n",
    "from scipy import misc\n",
    "def guess_digit(nn, sample):\n",
    "    prediction = nn.feedForward(sample)\n",
    "    return np.argmax(prediction)\n",
    "\n",
    "dirname = 'Images_test'\n",
    "\n",
    "for image in os.listdir(dirname):\n",
    "    data = misc.imread(os.path.join(dirname, image), flatten=True)\n",
    "    resized = misc.imresize(data, (28,28))\n",
    "    print(guess_digit(my_mnist_net, resized))\n",
    "    #ata = data.reshape((28,28))\n",
    "    #np_f = np.array(data) / 255.0\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training has been made on images that have a black background and a white digit, in the folder *Images_test* only one picture respects that format, that's the main reason of the wrong classification. A possible solution is to recognize the images with a white background (histogram of colors) and invert the colors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>Part 2</b>: Change the neural network structure and parameters to optimize performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b> Question 2.2.1</b>: Change the learning rate (0.001, 0.1, 1.0 , 10). Train the new neural nets with the original specifications (Part 2.1), for 50 iterations. \n",
    "Plot test accuracy vs iteration for each learning rate on the same graph. Report the maximum\n",
    "test accuracy achieved for each learning rate. Which one achieves the maximum test accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  1/50[==============] -Error: 0.4914513886  -Training_Accuracy:  38.11  -time: 19.72 \n",
      "('Validation accuracy', 39.5)\n",
      "Iteration:  2/50[==============] -Error: 0.3882449398  -Training_Accuracy:  47.52  -time: 40.07 \n",
      "('Validation accuracy', 48.89)\n",
      "Iteration:  3/50[==============] -Error: 0.3575008189  -Training_Accuracy:  52.20  -time: 60.30 \n",
      "('Validation accuracy', 53.449999999999996)\n",
      "Iteration:  4/50[==============] -Error: 0.3350249801  -Training_Accuracy:  55.98  -time: 82.47 \n",
      "('Validation accuracy', 56.97)\n",
      "Iteration:  5/50[==============] -Error: 0.3175562441  -Training_Accuracy:  59.03  -time: 104.19 \n",
      "('Validation accuracy', 60.019999999999996)\n",
      "Iteration:  6/50[==============] -Error: 0.3031632565  -Training_Accuracy:  61.71  -time: 125.09 \n",
      "('Validation accuracy', 62.480000000000004)\n",
      "Iteration:  7/50[==============] -Error: 0.2908094982  -Training_Accuracy:  64.54  -time: 151.53 \n",
      "('Validation accuracy', 65.36)\n"
     ]
    }
   ],
   "source": [
    "#Your implementation with a learning rate of 0.001 goes here \n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "my_mnist_net_1 = NeuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate=learning_rate)\n",
    "val_accuracies_1 = my_mnist_net_1.train(training_data,validation_data)\n",
    "test_accuracy.append(my_mnist_net_1.predict(test_data)/100)\n",
    "\n",
    "print('Learning rate %f, Test_Accuracy  %-2.2f' % (learning_rate, test_accuracy[-1]))\n",
    "\n",
    "my_mnist_net_1.save('NN_MNIST_0-001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plot_curve(range(1,my_mnist_net_1.iterations+1), val_accuracies_1, \"Val_Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Your implementation with a learning rate of 1.0 goes here \n",
    "\n",
    "learning_rate = 1.0\n",
    "\n",
    "my_mnist_net_2 = NeuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate=learning_rate)\n",
    "val_accuracies_2 = my_mnist_net_2.train(training_data,validation_data)\n",
    "test_accuracy.append(my_mnist_net_2.predict(test_data)/100)\n",
    "\n",
    "print('Learning rate %f, Test_Accuracy  %-2.2f' % (learning_rate, test_accuracy[-1]))\n",
    "\n",
    "my_mnist_net_2.save('NN_MNIST_1-00')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plot_curve(range(1,my_mnist_net_2.iterations+1), val_accuracies_2, \"Val_Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Your implementation with a learning rate of 10 goes here \n",
    "\n",
    "\n",
    "learning_rate = 10\n",
    "\n",
    "my_mnist_net_3 = NeuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate=learning_rate)\n",
    "val_accuracies_3 = my_mnist_net_3.train(training_data,validation_data)\n",
    "test_accuracy.append(my_mnist_net_3.predict(test_data)/100)\n",
    "print('Learning rate %f, Test_Accuracy  %-2.2f' % (learning_rate, test_accuracy[-1]))\n",
    "my_mnist_net_3.save('NN_MNIST_10-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plot_curve(range(1,my_mnist_net_3.iterations+1), val_accuracies_3, \"Val_Accuracy\")\n",
    "\n",
    "\n",
    "#plotting different validation accuracies in the same plot\n",
    "x = arange(0,50)\n",
    "plt.figure(figsize=(18,10))\n",
    "plt.title('Validation accuracies per different learning rates')\n",
    "plt.xticks(x)\n",
    "plt.grid()\n",
    "plt.plot(x,val_accuracies_1, label=0.001)\n",
    "plt.plot(x,val_accuracies, label=0.1)\n",
    "plt.plot(x,val_accuracies_2, label=1)\n",
    "plt.plot(x,val_accuracies_3, label=10)\n",
    "plt.legend(title=\"Learning rates\")\n",
    "plt.show()\n",
    "\n",
    "rates = [0.1, 0.001, 1, 10]\n",
    "\n",
    "best_rate = rates[np.argmax(test_accuracy)]\n",
    "print(\"Best learning rate = %f\", best_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h3>Comment</h3>\n",
    "Increasing the learning rate increases the step size at which our weights get updated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    " <b> Question 2.2.2 : </b> initialize all weights to 0.  Plot the training accuracy curve.\n",
    "Comment your results\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Your implementation goes here\n",
    "my_mnist_net_zeros = NeuralNetwork(input_nodes, hidden_nodes, output_nodes)\n",
    "\n",
    "input_hidden_weights = np.zeros(my_mnist_net_zeros.input, my_mnist_net_zeros.hidden-1)\n",
    "hidden_output_weights = np.zeros(my_mnist_net_zeros.hidden, my_mnist_net_zeros.output)\n",
    "\n",
    "my_mnist_net_zeros.weights_initialisation(input_hidden_weights, hidden_output_weights)\n",
    "\n",
    "my_mnist_net_zeros.train(training_data,validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#Your answer goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b> Question 2.2.3 : </b> Try with a different transfer function (such as tanh).\n",
    " File transfer_functions.py provides you the python implementation of the tanh function and its derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Your implementation goes here\n",
    "\n",
    "my_mnist_net_tanh = NeuralNetwork(input_nodes, hidden_nodes, output_nodes, transfer='tanh')\n",
    "#Your implementation goes here\n",
    "my_mnist_net_tanh.train(training_data,validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The range of values of *tanh* goes is (-1, 1) but our labels are only 0,1. That implies an unbalanced distribution of the labels [???] #JONAS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "<b> Question 2.2.4 : </b>  Add more neurons in the hidden layer (try with 100, 200, 300). Plot the curve representing the validation accuracy versus the number of neurons in the hidden layer.  (Choose and justify other hyper-parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Your implementation goes here\n",
    "\n",
    "learning_rate = best_rate\n",
    "#using the best rate found before\n",
    "my_mnist_net_100_hidden = NeuralNetwork(input_nodes, 100, output_nodes, learning_rate=learning_rate)\n",
    "val_accuracies_100 = my_mnist_net_100_hidden.train(training_data,validation_data)\n",
    "print(val_accuracies_100[-1])\n",
    "\n",
    "my_mnist_net_200_hidden = NeuralNetwork(input_nodes, 200, output_nodes, learning_rate=learning_rate)\n",
    "val_accuracies_200 = my_mnist_net_200_hidden.train(training_data,validation_data)\n",
    "print(val_accuracies_200[-1])\n",
    "\n",
    "my_mnist_net_300_hidden = NeuralNetwork(input_nodes, 300, output_nodes, learning_rate=learning_rate)\n",
    "val_accuracies_300 = my_mnist_net_300_hidden.train(training_data,validation_data)\n",
    "print(val_accuracies_300[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.xlabel(\"Number of hidden neurons\")\n",
    "plt.ylabel(\"Validation accuracy\")\n",
    "plt.title(\"Validation accuracy vs number of neurons in hidder layer\")\n",
    "plt.figure(figsize=(18,10))\n",
    "plt.plot([100, 200, 300], [val_accuracies_100[-1], val_accuracies_200[-1], val_accuracies_300[-1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#Your answer goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b> Question 2.2.5 : </b> Add one additionnal hidden layers and train your network, discuss your results with different setting. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Your implementation goes here\n",
    "\n",
    "\n",
    "import NeuralNetwork2 import NeuralNetwork2\n",
    "\n",
    "#create the network\n",
    "\n",
    "input_nodes = 784\n",
    "hidden_nodes_1 = 30\n",
    "hidden_nodes_2 = 30\n",
    "output_nodes = 10\n",
    "\n",
    "my_mnist_net = NeuralNetwork2(input_nodes, hidden_nodes_1, hidden_nodes_2, output_nodes)\n",
    "val_acc_1 = my_mnist_net.train(training_data,validation_data)\n",
    "acc_1 = my_mnist_net.predict(test_data)/100\n",
    "print(\"Accuracy with default learning rate = %f\" % acc_1)\n",
    "\n",
    "if best_rate != 0.1\n",
    "    my_mnist_net_2 = my_mnist_net = NeuralNetwork2(input_nodes, hidden_nodes_1, hidden_nodes_2, output_nodes, learning_rate = best_rate)\n",
    "    val_acc_2 = my_mnist_net_2.train(training_data, validation_data)\n",
    "    acc_2 = my_mnist_net_2.predict(test_data)/100\n",
    "    print(\"Learing rate = %f, accuracy = %f\" % (best_rate, acc_1))\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#Your answer goes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
